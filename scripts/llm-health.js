require('dotenv').config();
const { chatComplete } = require('../bot/llm');

(async () => {
  const provider = process.env.OPENAI_BASE_URL ? 'OpenRouter-compatible' : 'OpenAI';
  const baseURL = process.env.OPENAI_BASE_URL || 'https://api.openai.com/v1';
  const model = process.env.OPENAI_MODEL || 'gpt-4o-mini';
  const key = process.env.OPENAI_API_KEY || '';

  console.log('üîé LLM health check');
  console.log(`‚Ä¢ Provider: ${provider}`);
  console.log(`‚Ä¢ Base URL: ${baseURL}`);
  console.log(`‚Ä¢ Model: ${model}`);
  console.log(`‚Ä¢ Key present: ${key ? 'yes' : 'no'}`);

  if (!key) {
    console.error('‚ùå No OPENAI_API_KEY provided. Set your key in .env and re-run.');
    process.exit(1);
  }

  try {
    const answer = await chatComplete('Say "ok" if you can read this.', [], '');
    if (answer && /ok/i.test(answer)) {
      console.log('‚úÖ LLM responded:', answer);
      process.exit(0);
    } else if (answer) {
      console.log('üü® LLM responded (unexpected text):', answer);
      process.exit(0);
    } else {
      console.error('‚ùå LLM returned empty response. Check your key, base URL, and model.');
      console.error('   - If using OpenRouter, set OPENAI_BASE_URL=https://openrouter.ai/api/v1');
      console.error('   - Ensure model ID matches the provider (e.g., openai/gpt-4o-mini for OpenRouter)');
      process.exit(2);
    }
  } catch (e) {
    console.error('‚ùå LLM call threw an error:', e?.message || e);
    process.exit(3);
  }
})();
